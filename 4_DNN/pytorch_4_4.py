# -*- coding: utf-8 -*-
"""pytorch_4_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n4HGAlcUc9r5uAOECVC5D36cKOoIJitA
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import transforms, datasets

USE_CUDA = torch.cuda.is_available()
DEVICE = torch.device("cuda" if USE_CUDA else "cpu")

EPOCHS = 50
BATCH_SIZE = 64

train_loader = torch.utils.data.DataLoader(
    datasets.MNIST('./.data',
                  train = True,
                  download = True,
                  transform=transforms.Compose([
                                                transforms.RandomHorizontalFlip(),
                                                transforms.ToTensor(),
                                                transforms.Normalize((0.1307,), (0.3081,))
                  ])),
                  batch_size=BATCH_SIZE, shuffle= True
)

test_loader = torch.utils.data.DataLoader(
    datasets.MNIST('./.data',
                   train=False,
                   transform=transforms.Compose([
                                                 transforms.ToTensor(),
                                                 transforms.Normalize((0.1307,), (0.3081,))
                   ])),
                   batch_size=BATCH_SIZE, shuffle=True
)

class Net(nn.Module) :
  def __init__(self, dropout_p=0.2) :
    super(Net, self).__init__()
    self.fc1 = nn.Linear(784,256)
    self.fc2 = nn.Linear(256,128)
    self.fc3 = nn.Linear(128,10)
    self.dropout_p = dropout_p
  
  def forward(self, x) :
    x = x.view(-1,784)
    x = F.relu(self.fc1(x))
    x = F.dropout(x, training=self.training, p=self.dropout_p)
    x = F.relu(self.fc2(x))
    x = F.dropout(x,training=self.training,p=self.dropout_p)
    x = self.fc3(x)
    return x

model = Net(dropout_p=0.2).to(DEVICE)
optimizer = optim.SGD(model.parameters(), lr=0.01)

def train(model, train_loader, optimizer) :
  model.train()
  for batch_ids, (data,target) in enumerate(train_loader) :
    data, target = data.to(DEVICE), target.to(DEVICE)
    optimizer.zero_grad()
    output = model(data)
    loss = F.cross_entropy(output,target)
    loss.backward()
    optimizer.step()

def evaluate(model, test_loader) :
  model.eval()
  test_loss = 0
  correct = 0
  with torch.no_grad() :
    for data, target in test_loader :
      data, target = data.to(DEVICE), target.to(DEVICE)
      output = model(data)
      test_loss += F.cross_entropy(output, target, reduction='sum').item()
      pred = output.max(1,keepdim=True)[1]
      correct += pred.eq(target.view_as(pred)).sum().item()
  test_loss /= len(test_loader.dataset)
  test_accuracy = 100. * correct / len(test_loader.dataset)
  return test_loss, test_accuracy

for epoch in range(1, EPOCHS + 1) :
  train(model, train_loader, optimizer)
  test_loss, test_accuracy = evaluate(model, test_loader)

  print('[{}] Test Loss: {:.4f}, Accuracy: {:2f}%'.format(epoch, test_loss, test_accuracy))

