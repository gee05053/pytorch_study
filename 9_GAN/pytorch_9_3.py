# -*- coding: utf-8 -*-
"""pytorch_9.3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cXUKuHYcjSHoTMY8fI4m5Sp8ANozh1XD
"""

import os
import torch
import torchvision
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms, datasets
from torchvision.utils import save_image
import matplotlib.pyplot as plt
import numpy as np

EPOCHS = 300
BATCH_SIZE = 100
USE_CUDA = torch.cuda.is_available()
DEVICE = torch.device("cuda" if USE_CUDA else "cpu")
print("Using Device :", DEVICE)

trainset = datasets.FashionMNIST('./.data',
                                 train = True,
                                 download = True,
                                 transform = transforms.Compose([
                                                                 transforms.ToTensor(),
                                                                 transforms.Normalize((0.5,), (0.5,))
                                 ])
)

train_loader = torch.utils.data.DataLoader(
    dataset = trainset,
    batch_size = BATCH_SIZE,
    shuffle = True
)

class Generator(nn.Module) :
  def __init__(self) :
    super().__init__()
    self.embed = nn.Embedding(10,10)
    self.model = nn.Sequential(
        nn.Linear(110,256),
        nn.LeakyReLU(0.2, inplace=True),
        nn.Linear(256,512),
        nn.LeakyReLU(0.2, inplace=True),
        nn.Linear(512,1024),
        nn.LeakyReLU(0.2, inplace=True),
        nn.Linear(1024,784),
        nn.Tanh(),
    )

  def forward(self, z, labels) :
    c = self.embed(labels)
    x = torch.cat([z,c], 1)
    return self.model(x)

class Discriminator(nn.Module) :
  def __init__(self) :
    super().__init__()
    self.embed = nn.Embedding(10, 10)
    self.model = nn.Sequential(
        nn.Linear(794, 1024),
        nn.LeakyReLU(0.2, inplace=True),
        nn.Dropout(0.3),
        nn.Linear(1024, 512),
        nn.LeakyReLU(0.2, inplace=True),
        nn.Dropout(0.3),
        nn.Linear(512,256),
        nn.LeakyReLU(0.2, inplace=True),
        nn.Dropout(0.3),
        nn.Linear(256,1),
        nn.Sigmoid()
    )

  def forward(self, x, labels) :
    c = self.embed(labels)
    x = torch.cat([x,c], 1)
    return self.model(x)

D = Discriminator().to(DEVICE)
G = Generator().to(DEVICE)

criterion = nn.BCELoss()
d_optimizer = optim.Adam(D.parameters(), lr=0.0002)
g_optimizer = optim.Adam(G.parameters(), lr=0.0002)

total_step = len(train_loader)
for epoch in range(EPOCHS) :
  for i, (images, labels) in enumerate(train_loader) :
    images = images.reshape(BATCH_SIZE, -1).to(DEVICE)
    real_labels = torch.ones(BATCH_SIZE, 1).to(DEVICE)
    fake_labels = torch.zeros(BATCH_SIZE, 1).to(DEVICE)
    labels = labels.to(DEVICE)
    outputs = D(images, labels)
    d_loss_real = criterion(outputs, real_labels)
    real_score = outputs

    z = torch.randn(BATCH_SIZE, 100).to(DEVICE)
    g_label = torch.randint(0, 10, (BATCH_SIZE,)).to(DEVICE)
    fake_images = G(z, g_label)

    outputs = D(fake_images, g_label)
    d_loss_fake = criterion(outputs, fake_labels)
    fake_score = outputs

    d_loss = d_loss_real + d_loss_fake

    d_optimizer.zero_grad()
    g_optimizer.zero_grad()
    d_loss.backward()
    d_optimizer.step()

    fake_images = G(z, g_label)
    outputs = D(fake_images, g_label)
    g_loss = criterion(outputs, real_labels)

    d_optimizer.zero_grad()
    g_optimizer.zero_grad()
    g_loss.backward()
    g_optimizer.step()
  print('이폭 [{}/{}] d_loss:{:.4f} g_loss:{:.4f} D(x):{:.2f} D(G(z)):{:.2f}'.format(epoch, EPOCHS, d_loss.item(), g_loss.item(), real_score.mean().item(), fake_score.mean().item()))

item_number = 9
z = torch.randn(1,100).to(DEVICE)
g_label = torch.full((1,), item_number, dtype=torch.long).to(DEVICE)

sample_images = G(z,g_label)

sample_images_img = np.reshape(sample_images.data.cpu().numpy()[0], (28,28))
plt.imshow(sample_images_img, cmap='gray')
plt.show()

